{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_transition(x):\n",
    "    x1 = x[0] **2\n",
    "    x2 = x[1] **3\n",
    "    x3 = x[2] **4\n",
    "    x4 = torch.sin(x[3])\n",
    "    reward = x.sum() #Linear Reward (can be quadratic)\n",
    "    return (torch.tensor([x1, x2, x3, x4]), torch.tensor([reward]))\n",
    "train = []\n",
    "test = []\n",
    "N = 1000\n",
    "for i in range(N):\n",
    "    x = torch.randn(4)\n",
    "    y, r = non_linear_transition(x)\n",
    "    if i % 10 == 0:\n",
    "        test.append((x, y,r))\n",
    "    else:\n",
    "        train.append((x, y, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dim = 16\n",
    "state_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, enc_dim//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim//2, enc_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim, enc_dim),\n",
    ")\n",
    "\n",
    "state_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(enc_dim, enc_dim//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim//2, 4)\n",
    ")\n",
    "\n",
    "A = torch.nn.Parameter(torch.randn(enc_dim, enc_dim))\n",
    "Q = torch.nn.Parameter(torch.randn(enc_dim, enc_dim))\n",
    "optimizer = torch.optim.Adam(list(state_encoder.parameters()) + list(state_decoder.parameters()) + [A] + [Q], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikef\\AppData\\Local\\Temp\\ipykernel_8444\\4282157989.py:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_e1nmgx0bk6\\croot\\pytorch-select_1725478824526\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  pred_reward = xx.T @ Q_psd @ xx\n",
      "c:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train State Loss: 23.948470457628368, Train Reward Loss: 337.1558699510098, Test State Loss: 30.52271986424923, Test Reward Loss: 420.91455726623536\n",
      "Epoch 10: Train State Loss: 4.490297100752592, Train Reward Loss: 2839.6854546813965, Test State Loss: 5.8039762602746485, Test Reward Loss: 3056.0170069885253\n",
      "Epoch 20: Train State Loss: 3.1285800026506188, Train Reward Loss: 3856.889682495117, Test State Loss: 3.9753887915611266, Test Reward Loss: 4656.601389312744\n",
      "Epoch 30: Train State Loss: 2.5718757000640036, Train Reward Loss: 4279.302886932373, Test State Loss: 3.131368246078491, Test Reward Loss: 4872.341660308838\n",
      "Epoch 40: Train State Loss: 2.0009902143627407, Train Reward Loss: 3165.4726855010986, Test State Loss: 2.0337191195786, Test Reward Loss: 3402.8020547485353\n",
      "Epoch 50: Train State Loss: 1.7177786374166608, Train Reward Loss: 2755.3142679290772, Test State Loss: 2.85171800032258, Test Reward Loss: 2996.986884918213\n",
      "Epoch 60: Train State Loss: 1.7208269734680652, Train Reward Loss: 2184.7905327911376, Test State Loss: 2.577941499501467, Test Reward Loss: 2682.488246307373\n",
      "Epoch 70: Train State Loss: 1.4806481339670718, Train Reward Loss: 1710.252405593872, Test State Loss: 3.4889750720560553, Test Reward Loss: 2083.1802911376953\n",
      "Epoch 80: Train State Loss: 1.5126839772388339, Train Reward Loss: 1675.0041296844483, Test State Loss: 2.6214778554439544, Test Reward Loss: 2298.7417025756836\n",
      "Epoch 90: Train State Loss: 1.4118032827340066, Train Reward Loss: 1745.9600689086915, Test State Loss: 1.6062883298285306, Test Reward Loss: 1992.973039855957\n",
      "Epoch 100: Train State Loss: 1.371388355500996, Train Reward Loss: 1624.4008784942628, Test State Loss: 1.6345398066192864, Test Reward Loss: 2090.8149362182617\n",
      "Epoch 110: Train State Loss: 0.9927489501982927, Train Reward Loss: 1624.0097161560059, Test State Loss: 1.927342959716916, Test Reward Loss: 2045.6726379394531\n",
      "Epoch 120: Train State Loss: 0.9220847277268768, Train Reward Loss: 1887.400250152588, Test State Loss: 1.3274855513125658, Test Reward Loss: 2081.4006552124024\n",
      "Epoch 130: Train State Loss: 0.9260067533664406, Train Reward Loss: 1690.4838901672363, Test State Loss: 1.390474630445242, Test Reward Loss: 2044.0851843261719\n",
      "Epoch 140: Train State Loss: 0.8659033946655691, Train Reward Loss: 1447.2222908935546, Test State Loss: 1.5125991704314947, Test Reward Loss: 1599.5919671630859\n",
      "Epoch 150: Train State Loss: 0.9846197149679065, Train Reward Loss: 1419.6758175964355, Test State Loss: 1.496016826890409, Test Reward Loss: 1609.8688107299804\n",
      "Epoch 160: Train State Loss: 0.8726831729598343, Train Reward Loss: 1303.2765552978515, Test State Loss: 1.062527740597725, Test Reward Loss: 1426.55789352417\n",
      "Epoch 170: Train State Loss: 0.8278510490842164, Train Reward Loss: 1259.904403717041, Test State Loss: 1.054001142606139, Test Reward Loss: 1447.3640545654298\n",
      "Epoch 180: Train State Loss: 0.8778587335292249, Train Reward Loss: 1120.2496395111084, Test State Loss: 1.0987484227865933, Test Reward Loss: 1245.0990490722656\n",
      "Epoch 190: Train State Loss: 0.8957564590349794, Train Reward Loss: 1021.4222609100342, Test State Loss: 1.4278271372243763, Test Reward Loss: 1155.1257090759277\n",
      "Epoch 200: Train State Loss: 0.888483590638265, Train Reward Loss: 1022.6309409637452, Test State Loss: 1.8370299132168293, Test Reward Loss: 1259.3716206359863\n",
      "Epoch 210: Train State Loss: 0.7923664513342082, Train Reward Loss: 865.6480180664063, Test State Loss: 1.4220730606094003, Test Reward Loss: 997.7176081848145\n",
      "Epoch 220: Train State Loss: 0.8461761198043823, Train Reward Loss: 792.3777254180908, Test State Loss: 1.816968145519495, Test Reward Loss: 894.0099047851562\n",
      "Epoch 230: Train State Loss: 1.007117860423401, Train Reward Loss: 634.4778247451782, Test State Loss: 1.699769198819995, Test Reward Loss: 702.1411189270019\n",
      "Epoch 240: Train State Loss: 1.0692565016411244, Train Reward Loss: 595.0262225112915, Test State Loss: 1.2780534315481782, Test Reward Loss: 730.0657588195801\n",
      "Epoch 250: Train State Loss: 0.7823983194343745, Train Reward Loss: 570.3248346481323, Test State Loss: 1.5970052950829268, Test Reward Loss: 618.7627265930175\n",
      "Epoch 260: Train State Loss: 0.8857809895332903, Train Reward Loss: 521.1965414276123, Test State Loss: 1.4551722599565984, Test Reward Loss: 674.8363429260254\n",
      "Epoch 270: Train State Loss: 0.700944150639698, Train Reward Loss: 458.94880000305176, Test State Loss: 1.3904055028408766, Test Reward Loss: 485.97022705078126\n",
      "Epoch 280: Train State Loss: 0.7634713716097176, Train Reward Loss: 446.54269794464113, Test State Loss: 1.4331030691415072, Test Reward Loss: 483.3905017089844\n",
      "Epoch 290: Train State Loss: 0.8582606464792043, Train Reward Loss: 455.2104765930176, Test State Loss: 1.883806093260646, Test Reward Loss: 588.4176707458496\n",
      "Epoch 300: Train State Loss: 0.9036405920255929, Train Reward Loss: 359.93387255096434, Test State Loss: 1.6903456608206033, Test Reward Loss: 410.10062545776367\n",
      "Epoch 310: Train State Loss: 0.9043981374911964, Train Reward Loss: 379.44487244415285, Test State Loss: 1.6065708875842393, Test Reward Loss: 453.14411682128906\n",
      "Epoch 320: Train State Loss: 0.7553325633555651, Train Reward Loss: 344.122634853363, Test State Loss: 1.6392172246426344, Test Reward Loss: 387.40291900634764\n",
      "Epoch 330: Train State Loss: 0.9344837592858821, Train Reward Loss: 361.157164604187, Test State Loss: 1.508186919875443, Test Reward Loss: 371.9249677276611\n",
      "Epoch 340: Train State Loss: 0.7765741322673857, Train Reward Loss: 367.53562225723266, Test State Loss: 1.3158566832914949, Test Reward Loss: 383.57801162719727\n",
      "Epoch 350: Train State Loss: 0.7246044174470008, Train Reward Loss: 343.844467010498, Test State Loss: 2.196745662391186, Test Reward Loss: 410.0248692321777\n",
      "Epoch 360: Train State Loss: 0.6920955485384911, Train Reward Loss: 320.5368977775574, Test State Loss: 2.288435120098293, Test Reward Loss: 421.4581220245361\n",
      "Epoch 370: Train State Loss: 0.7623813605327159, Train Reward Loss: 341.8974568977356, Test State Loss: 2.073639338761568, Test Reward Loss: 409.8728918457031\n",
      "Epoch 380: Train State Loss: 0.689900058357045, Train Reward Loss: 360.0709126281738, Test State Loss: 1.41817712970078, Test Reward Loss: 342.8708142852783\n",
      "Epoch 390: Train State Loss: 0.7702254818249494, Train Reward Loss: 339.1616493148804, Test State Loss: 1.7833146383613347, Test Reward Loss: 408.7871631622314\n",
      "Epoch 400: Train State Loss: 0.7827762173563242, Train Reward Loss: 327.0417686500549, Test State Loss: 1.7709039842523635, Test Reward Loss: 392.37516815185546\n",
      "Epoch 410: Train State Loss: 0.7028217700757087, Train Reward Loss: 312.814802942276, Test State Loss: 1.321175962537527, Test Reward Loss: 325.0704860687256\n",
      "Epoch 420: Train State Loss: 0.6698250985927879, Train Reward Loss: 384.1239982070923, Test State Loss: 1.485824468024075, Test Reward Loss: 436.3519477844238\n",
      "Epoch 430: Train State Loss: 0.6651338291354477, Train Reward Loss: 316.49870397758485, Test State Loss: 1.1414480709750205, Test Reward Loss: 266.693106880188\n",
      "Epoch 440: Train State Loss: 0.7649356535989791, Train Reward Loss: 331.9107939815521, Test State Loss: 0.8136216816492379, Test Reward Loss: 402.18984024047853\n",
      "Epoch 450: Train State Loss: 0.8116782429739833, Train Reward Loss: 333.9515196456909, Test State Loss: 1.912613752000034, Test Reward Loss: 442.7991409301758\n",
      "Epoch 460: Train State Loss: 0.6579228699598462, Train Reward Loss: 378.4630412120819, Test State Loss: 1.3746882376633585, Test Reward Loss: 389.41977931976317\n",
      "Epoch 470: Train State Loss: 0.9492184875477105, Train Reward Loss: 414.80981857299804, Test State Loss: 1.3570779731869698, Test Reward Loss: 377.0196224594116\n",
      "Epoch 480: Train State Loss: 0.6769138956330717, Train Reward Loss: 406.8891463623047, Test State Loss: 2.3848993320390584, Test Reward Loss: 596.7573323822021\n",
      "Epoch 490: Train State Loss: 0.755805185591802, Train Reward Loss: 433.4600323905945, Test State Loss: 1.8397798567637802, Test Reward Loss: 661.841029548645\n",
      "Epoch 500: Train State Loss: 0.6534314652718604, Train Reward Loss: 453.21954039382933, Test State Loss: 1.6446398221701384, Test Reward Loss: 505.90124221801756\n",
      "Epoch 510: Train State Loss: 0.735219357073307, Train Reward Loss: 441.977438659668, Test State Loss: 1.5440717754885553, Test Reward Loss: 544.5188540649414\n",
      "Epoch 520: Train State Loss: 0.6698363079428673, Train Reward Loss: 390.65535845184326, Test State Loss: 2.076233638934791, Test Reward Loss: 430.5683576202393\n",
      "Epoch 530: Train State Loss: 0.8343333600237965, Train Reward Loss: 429.63351583099364, Test State Loss: 1.4490399499982596, Test Reward Loss: 529.7258488082886\n",
      "Epoch 540: Train State Loss: 0.6442238495834172, Train Reward Loss: 407.59334339523315, Test State Loss: 1.2277893653512002, Test Reward Loss: 500.2405842971802\n",
      "Epoch 550: Train State Loss: 0.6578109453991056, Train Reward Loss: 401.21995537948607, Test State Loss: 1.1923771187476815, Test Reward Loss: 351.98545833587644\n",
      "Epoch 560: Train State Loss: 0.621721488378942, Train Reward Loss: 412.2696494655609, Test State Loss: 1.0331384363770484, Test Reward Loss: 400.94203216552734\n",
      "Epoch 570: Train State Loss: 0.763092808721587, Train Reward Loss: 378.06444203567503, Test State Loss: 1.3781993908807635, Test Reward Loss: 427.2899433517456\n",
      "Epoch 580: Train State Loss: 0.7669946748279035, Train Reward Loss: 363.67400890541074, Test State Loss: 1.363027858659625, Test Reward Loss: 358.3317945480347\n",
      "Epoch 590: Train State Loss: 0.6750614606682211, Train Reward Loss: 383.09093432807924, Test State Loss: 1.6815910507924854, Test Reward Loss: 519.019747581482\n",
      "Epoch 600: Train State Loss: 0.661375175582245, Train Reward Loss: 419.04071284484866, Test State Loss: 1.247385134063661, Test Reward Loss: 405.37716400146485\n",
      "Epoch 610: Train State Loss: 0.7643635943289846, Train Reward Loss: 377.18213794326783, Test State Loss: 1.0290239189378918, Test Reward Loss: 457.47683513641357\n",
      "Epoch 620: Train State Loss: 0.6313554168976844, Train Reward Loss: 369.36416529083255, Test State Loss: 1.5732263243570923, Test Reward Loss: 384.25055862426757\n",
      "Epoch 630: Train State Loss: 0.7061567778456956, Train Reward Loss: 354.15924522209167, Test State Loss: 1.400563204139471, Test Reward Loss: 363.8110542678833\n",
      "Epoch 640: Train State Loss: 0.6549556102007628, Train Reward Loss: 361.0689929943085, Test State Loss: 1.5330568090081216, Test Reward Loss: 365.97343709945676\n",
      "Epoch 650: Train State Loss: 0.6965846651121974, Train Reward Loss: 429.171833190918, Test State Loss: 1.5922841580584646, Test Reward Loss: 405.37281511306765\n",
      "Epoch 660: Train State Loss: 0.7682763820681721, Train Reward Loss: 335.60361156082155, Test State Loss: 1.2940637814253568, Test Reward Loss: 336.19311609268186\n",
      "Epoch 670: Train State Loss: 0.7838426686506719, Train Reward Loss: 329.7275038127899, Test State Loss: 1.2015220308303833, Test Reward Loss: 443.85187301635744\n",
      "Epoch 680: Train State Loss: 0.667513054240495, Train Reward Loss: 336.2826026763916, Test State Loss: 1.5864574782550336, Test Reward Loss: 401.9709702301025\n",
      "Epoch 690: Train State Loss: 0.6263633155543357, Train Reward Loss: 415.39984182929993, Test State Loss: 1.5691800165548921, Test Reward Loss: 411.5740305328369\n",
      "Epoch 700: Train State Loss: 0.5710755709558726, Train Reward Loss: 429.00560777282715, Test State Loss: 1.915881851837039, Test Reward Loss: 555.7093995285035\n",
      "Epoch 710: Train State Loss: 0.8080472925901413, Train Reward Loss: 431.03114707946776, Test State Loss: 2.1308436482213438, Test Reward Loss: 475.71704471588134\n",
      "Epoch 720: Train State Loss: 0.6192944407127797, Train Reward Loss: 415.6407051868439, Test State Loss: 1.7820163828134536, Test Reward Loss: 443.6128964996338\n",
      "Epoch 730: Train State Loss: 0.6371361865438521, Train Reward Loss: 397.2649708557129, Test State Loss: 3.746484768241644, Test Reward Loss: 524.7394898605347\n",
      "Epoch 740: Train State Loss: 0.6819729754328727, Train Reward Loss: 475.6727876968384, Test State Loss: 1.9593536849692463, Test Reward Loss: 524.4545353317261\n",
      "Epoch 750: Train State Loss: 0.5919513872731477, Train Reward Loss: 423.03131689071654, Test State Loss: 1.741338133662939, Test Reward Loss: 439.69211322784423\n",
      "Epoch 760: Train State Loss: 0.6074166649598628, Train Reward Loss: 553.7308689041138, Test State Loss: 2.1794085512310266, Test Reward Loss: 637.2873357009888\n",
      "Epoch 770: Train State Loss: 0.6584231908544897, Train Reward Loss: 499.3686135139465, Test State Loss: 3.463146325647831, Test Reward Loss: 821.8743603897095\n",
      "Epoch 780: Train State Loss: 0.6019198948480189, Train Reward Loss: 493.5188420600891, Test State Loss: 1.8407961284741758, Test Reward Loss: 640.410288887024\n",
      "Epoch 790: Train State Loss: 0.8554441048502922, Train Reward Loss: 481.03394037246704, Test State Loss: 2.0692108612135054, Test Reward Loss: 584.0340757369995\n",
      "Epoch 800: Train State Loss: 0.6997474236283451, Train Reward Loss: 400.94546394348146, Test State Loss: 2.493171304874122, Test Reward Loss: 518.1716198730469\n",
      "Epoch 810: Train State Loss: 0.6220205107033253, Train Reward Loss: 382.4565164871216, Test State Loss: 2.5505081042274833, Test Reward Loss: 427.8620837020874\n",
      "Epoch 820: Train State Loss: 0.6053715685643256, Train Reward Loss: 441.5510796031952, Test State Loss: 2.467684662230313, Test Reward Loss: 420.42960041046143\n",
      "Epoch 830: Train State Loss: 0.6706569630224257, Train Reward Loss: 439.33935619354247, Test State Loss: 1.7715197217464447, Test Reward Loss: 515.061182975769\n",
      "Epoch 840: Train State Loss: 0.6052549765519798, Train Reward Loss: 542.5432048530579, Test State Loss: 1.9747683300822974, Test Reward Loss: 579.8184664154053\n",
      "Epoch 850: Train State Loss: 0.7895517235305161, Train Reward Loss: 459.5467811126709, Test State Loss: 1.4646148987486958, Test Reward Loss: 572.6350864028931\n",
      "Epoch 860: Train State Loss: 0.6405936615709216, Train Reward Loss: 454.1899471054077, Test State Loss: 2.3200806583836675, Test Reward Loss: 634.415185585022\n",
      "Epoch 870: Train State Loss: 0.5860436976188794, Train Reward Loss: 444.2919887123108, Test State Loss: 2.0898430878669023, Test Reward Loss: 456.16669048309325\n",
      "Epoch 880: Train State Loss: 0.6729665071740747, Train Reward Loss: 458.167266204834, Test State Loss: 0.9298450967669487, Test Reward Loss: 567.6498519134522\n",
      "Epoch 890: Train State Loss: 0.7417703374326229, Train Reward Loss: 425.29730448150633, Test State Loss: 1.879055690392852, Test Reward Loss: 502.27401851654054\n",
      "Epoch 900: Train State Loss: 0.6943664624802768, Train Reward Loss: 427.9620497207642, Test State Loss: 1.334471992328763, Test Reward Loss: 535.3812763595581\n",
      "Epoch 910: Train State Loss: 0.6227628744281828, Train Reward Loss: 422.2258187866211, Test State Loss: 1.8643936282396316, Test Reward Loss: 508.92587230682375\n",
      "Epoch 920: Train State Loss: 0.7607748242150992, Train Reward Loss: 428.5417607879639, Test State Loss: 2.5009285597875714, Test Reward Loss: 503.8476866912842\n",
      "Epoch 930: Train State Loss: 0.6563396088387817, Train Reward Loss: 382.59615693473813, Test State Loss: 1.7173915370553732, Test Reward Loss: 504.4056015586853\n",
      "Epoch 940: Train State Loss: 0.5949394075274468, Train Reward Loss: 374.4037623596191, Test State Loss: 1.780493438579142, Test Reward Loss: 465.0099817848206\n",
      "Epoch 950: Train State Loss: 0.5573766300473362, Train Reward Loss: 355.36315432548525, Test State Loss: 1.1910743394494057, Test Reward Loss: 432.4302230834961\n",
      "Epoch 960: Train State Loss: 0.6539295669272542, Train Reward Loss: 330.7278512210846, Test State Loss: 2.2165195913240314, Test Reward Loss: 382.86395280838013\n",
      "Epoch 970: Train State Loss: 0.6284218320138752, Train Reward Loss: 376.8732117786407, Test State Loss: 2.627784825898707, Test Reward Loss: 396.7263229942322\n",
      "Epoch 980: Train State Loss: 0.6917797099221498, Train Reward Loss: 300.2235312480926, Test State Loss: 2.456482640095055, Test Reward Loss: 379.899227809906\n",
      "Epoch 990: Train State Loss: 0.5978762227371335, Train Reward Loss: 301.42065577030183, Test State Loss: 1.6993702544271947, Test Reward Loss: 368.39304622650144\n",
      "Epoch 1000: Train State Loss: 0.5836877498188987, Train Reward Loss: 299.6776215362549, Test State Loss: 1.3950257007405162, Test Reward Loss: 282.2891312980652\n",
      "Epoch 1010: Train State Loss: 0.5488109988737851, Train Reward Loss: 235.23362493896485, Test State Loss: 1.5760105860605835, Test Reward Loss: 252.4122456741333\n",
      "Epoch 1020: Train State Loss: 0.5670616146195679, Train Reward Loss: 211.8567365088463, Test State Loss: 1.9336623485013842, Test Reward Loss: 250.72386289596557\n",
      "Epoch 1030: Train State Loss: 0.654658605625853, Train Reward Loss: 210.37013329696654, Test State Loss: 1.6447548021376133, Test Reward Loss: 316.291924571991\n",
      "Epoch 1040: Train State Loss: 0.6856689165644347, Train Reward Loss: 202.34750925064088, Test State Loss: 1.3423084499686957, Test Reward Loss: 249.17674047470092\n",
      "Epoch 1050: Train State Loss: 0.6097825695872306, Train Reward Loss: 208.50875461387633, Test State Loss: 1.7139310028403998, Test Reward Loss: 246.10713649749755\n",
      "Epoch 1060: Train State Loss: 0.634036867717281, Train Reward Loss: 158.5200654346943, Test State Loss: 1.6899182680621743, Test Reward Loss: 147.24665682792664\n",
      "Epoch 1070: Train State Loss: 0.6676582455337048, Train Reward Loss: 192.7719368391037, Test State Loss: 2.3546035846322777, Test Reward Loss: 186.35702486038207\n",
      "Epoch 1080: Train State Loss: 0.7001408942574635, Train Reward Loss: 174.99014690971376, Test State Loss: 1.6314858407527209, Test Reward Loss: 165.19391382217407\n",
      "Epoch 1090: Train State Loss: 0.6555229075476527, Train Reward Loss: 176.32767091989518, Test State Loss: 1.8962038614600898, Test Reward Loss: 205.37806159973144\n",
      "Epoch 1100: Train State Loss: 0.5632399711776525, Train Reward Loss: 186.1588914871216, Test State Loss: 1.0405203829705716, Test Reward Loss: 184.11329545021056\n",
      "Epoch 1110: Train State Loss: 0.7247683256687596, Train Reward Loss: 155.04321657657624, Test State Loss: 1.4400702103972436, Test Reward Loss: 203.6499815940857\n",
      "Epoch 1120: Train State Loss: 0.7631633418854326, Train Reward Loss: 142.42360295677184, Test State Loss: 1.3846279391646386, Test Reward Loss: 150.3556134223938\n",
      "Epoch 1130: Train State Loss: 0.5713165749013424, Train Reward Loss: 141.81861286735534, Test State Loss: 0.9089157211780549, Test Reward Loss: 169.75047189712524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m dec_state \u001b[38;5;241m=\u001b[39m state_decoder(pred_state)   \n\u001b[0;32m     14\u001b[0m state_og_loss \u001b[38;5;241m=\u001b[39m criterion(dec_state, y)\n\u001b[1;32m---> 15\u001b[0m state_enc_loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mstate_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, xx)\n\u001b[0;32m     16\u001b[0m auto_encode_loss \u001b[38;5;241m=\u001b[39m criterion(state_decoder(state_encoder(x)), x)\n\u001b[0;32m     17\u001b[0m state_loss \u001b[38;5;241m=\u001b[39m state_og_loss \u001b[38;5;241m+\u001b[39m state_enc_loss \u001b[38;5;241m+\u001b[39m auto_encode_loss\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train, test are lists of tuples of (x, y, r)\n",
    "epochs = 10000\n",
    "criterion = torch.nn.MSELoss()\n",
    "for i in range(epochs):\n",
    "    total_state_loss = 0\n",
    "    total_reward_loss = 0\n",
    "    for x, y, r in train:\n",
    "        optimizer.zero_grad()\n",
    "        xx = state_encoder(x)\n",
    "        pred_state = A @ xx\n",
    "        Q_psd = Q.T @ Q\n",
    "        pred_reward = xx.T @ Q_psd @ xx\n",
    "        dec_state = state_decoder(pred_state)   \n",
    "        state_og_loss = criterion(dec_state, y)\n",
    "        state_enc_loss = criterion(state_encoder(y), xx)\n",
    "        auto_encode_loss = criterion(state_decoder(state_encoder(x)), x)\n",
    "        state_loss = state_og_loss + state_enc_loss + auto_encode_loss\n",
    "        reward_loss = criterion(pred_reward, r)\n",
    "        loss = state_loss #+ reward_loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_state_loss += state_loss.item()\n",
    "        total_reward_loss += reward_loss.item()\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            total_test_state_loss = 0\n",
    "            total_test_reward_loss = 0\n",
    "            for x, y,r  in test:\n",
    "                xx = state_encoder(x)\n",
    "                pred_state = A @ xx\n",
    "                Q_psd = Q.T @ Q\n",
    "                pred_reward = xx.T @ Q_psd @ xx\n",
    "                dec_state = state_decoder(pred_state)\n",
    "                state_og_loss = criterion(dec_state, y)\n",
    "                state_enc_loss = criterion(state_encoder(y), xx)\n",
    "                auto_encode_loss = criterion(state_decoder(state_encoder(x)), x)\n",
    "                state_loss = state_og_loss + state_enc_loss + auto_encode_loss\n",
    "                reward_loss = criterion(pred_reward, r)\n",
    "                total_test_state_loss += state_loss.item()\n",
    "                total_test_reward_loss += reward_loss.item()\n",
    "            print(f\"Epoch {i}: Train State Loss: {total_state_loss/N}, Train Reward Loss: {total_reward_loss/N}, Test State Loss: {total_test_state_loss/len(test)}, Test Reward Loss: {total_test_reward_loss/len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3892, -1.1420,  0.4347,  0.3648])\n",
      "tensor([ 0.0351, -0.0070,  0.1394, -1.6561,  0.3626, -0.8260,  1.1684,  0.1074,\n",
      "         0.7020, -0.0528, -0.3366,  0.7623, -0.2779, -0.1570,  0.3622,  0.0454],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([ 0.1514, -1.4894,  0.0357,  0.3568])\n",
      "tensor([ 0.1218, -1.5987,  0.1711,  0.1662], grad_fn=<ViewBackward0>)\n",
      "Test\n",
      "tensor([ 0.6592, -0.8172, -1.5943, -2.5283])\n",
      "tensor([ 0.4345, -0.5458,  6.4610, -0.5755])\n",
      "tensor([ 0.3680, -0.9164,  6.7711, -1.4532], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = train[0]\n",
    "x, y, r = t\n",
    "xx = state_encoder(x)\n",
    "pred_state = A @ xx\n",
    "dec_state = state_decoder(pred_state)\n",
    "print(x)\n",
    "print(xx)\n",
    "print(y)\n",
    "print(dec_state)\n",
    "print(\"Test\")\n",
    "t = test[0]\n",
    "x, y,r = t\n",
    "xx = state_encoder(x)\n",
    "pred_state = A @ xx\n",
    "dec_state = state_decoder(pred_state)\n",
    "print(x)\n",
    "print(y)\n",
    "print(dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
