{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_transition(x):\n",
    "    x1 = x[0] **2\n",
    "    x2 = x[1] **3\n",
    "    x3 = x[2] **4\n",
    "    x4 = torch.sin(x[3])\n",
    "    reward = x.sum() #Linear Reward (can be quadratic)\n",
    "    return (torch.tensor([x1, x2, x3, x4]), torch.tensor([reward]))\n",
    "train = []\n",
    "test = []\n",
    "N = 1000\n",
    "for i in range(N):\n",
    "    x = torch.randn(4)\n",
    "    y, r = non_linear_transition(x)\n",
    "    if i % 10 == 0:\n",
    "        test.append((x, y,r))\n",
    "    else:\n",
    "        train.append((x, y, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dim = 16\n",
    "state_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, enc_dim//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim//2, enc_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim, enc_dim),\n",
    ")\n",
    "\n",
    "state_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(enc_dim, enc_dim//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(enc_dim//2, 4)\n",
    ")\n",
    "\n",
    "A = torch.nn.Parameter(torch.randn(enc_dim, enc_dim))\n",
    "Q = torch.nn.Parameter(torch.randn(enc_dim, enc_dim))\n",
    "optimizer = torch.optim.Adam(list(state_encoder.parameters()) + list(state_decoder.parameters()) + [A] + [Q], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train State Loss: 5.842352452687919, Train Reward Loss: 31.381448214975993, Test State Loss: 4.450371186621487, Test Reward Loss: 7.446117175092513\n",
      "Epoch 10: Train State Loss: 1.9988502228418366, Train Reward Loss: 2.228102112329458, Test State Loss: 1.6656546838767827, Test Reward Loss: 3.551352343352046\n",
      "Epoch 20: Train State Loss: 1.1206838486506603, Train Reward Loss: 2.104244506692847, Test State Loss: 1.0893463388457894, Test Reward Loss: 3.4069696409760946\n",
      "Epoch 30: Train State Loss: 0.7032914839796722, Train Reward Loss: 2.059865338200907, Test State Loss: 0.8657066050171852, Test Reward Loss: 3.368934662309475\n",
      "Epoch 40: Train State Loss: 0.8388995628934354, Train Reward Loss: 2.034564663586121, Test State Loss: 1.5374239451624454, Test Reward Loss: 3.328597938924006\n",
      "Epoch 50: Train State Loss: 0.8845783798075281, Train Reward Loss: 2.0142493329093702, Test State Loss: 2.9323431236017496, Test Reward Loss: 3.255285142699104\n",
      "Epoch 60: Train State Loss: 0.7355981693575159, Train Reward Loss: 1.974180449518024, Test State Loss: 1.2407171595469118, Test Reward Loss: 3.2172990227586795\n",
      "Epoch 70: Train State Loss: 0.7035362504152581, Train Reward Loss: 1.933288125845236, Test State Loss: 3.5997624457720665, Test Reward Loss: 3.157714663697334\n",
      "Epoch 80: Train State Loss: 0.9027788064787164, Train Reward Loss: 1.9094638200811367, Test State Loss: 1.2150651626149191, Test Reward Loss: 3.156324528695259\n",
      "Epoch 90: Train State Loss: 0.8496369263147935, Train Reward Loss: 1.903060867268112, Test State Loss: 0.9294335901457816, Test Reward Loss: 3.134287892462307\n",
      "Epoch 100: Train State Loss: 0.7386675640493632, Train Reward Loss: 1.8955462835196968, Test State Loss: 0.660837791338563, Test Reward Loss: 3.1093186835831874\n",
      "Epoch 110: Train State Loss: 0.6744453578526154, Train Reward Loss: 1.8874303751739117, Test State Loss: 2.706772584039718, Test Reward Loss: 3.0899825573223643\n",
      "Epoch 120: Train State Loss: 0.5846135934735648, Train Reward Loss: 1.888778372744499, Test State Loss: 2.4897545525431632, Test Reward Loss: 3.0878238262678495\n",
      "Epoch 130: Train State Loss: 0.807183177058585, Train Reward Loss: 1.8862178797532916, Test State Loss: 0.6332425873260945, Test Reward Loss: 3.104383021595786\n",
      "Epoch 140: Train State Loss: 0.7159060160606168, Train Reward Loss: 1.8889565818376322, Test State Loss: 0.8105573785491288, Test Reward Loss: 3.102401186898169\n",
      "Epoch 150: Train State Loss: 0.7707438495298847, Train Reward Loss: 1.8892711577132972, Test State Loss: 0.643389936760068, Test Reward Loss: 3.1034913029565363\n",
      "Epoch 160: Train State Loss: 0.5209850286536384, Train Reward Loss: 1.8761476382720719, Test State Loss: 1.9601037179585545, Test Reward Loss: 3.080886565427163\n",
      "Epoch 170: Train State Loss: 0.5803831977697554, Train Reward Loss: 1.868987089666113, Test State Loss: 0.8381781367678195, Test Reward Loss: 3.1063938273044185\n",
      "Epoch 180: Train State Loss: 0.9968263928531669, Train Reward Loss: 1.8623648560148294, Test State Loss: 0.3856077899504453, Test Reward Loss: 3.1329633481719066\n",
      "Epoch 190: Train State Loss: 0.4821185148889199, Train Reward Loss: 1.8584564525457183, Test State Loss: 0.5039411833137274, Test Reward Loss: 3.2231092094583436\n",
      "Epoch 200: Train State Loss: 0.5401695140842349, Train Reward Loss: 1.8566353048154192, Test State Loss: 1.0183564553130418, Test Reward Loss: 3.1882283195015044\n",
      "Epoch 210: Train State Loss: 0.4468335580229759, Train Reward Loss: 1.8565064060843308, Test State Loss: 0.5922318445704877, Test Reward Loss: 3.1232923776869574\n",
      "Epoch 220: Train State Loss: 0.3358032128978521, Train Reward Loss: 1.8543631255769368, Test State Loss: 0.9819039517594501, Test Reward Loss: 3.1571744923479854\n",
      "Epoch 230: Train State Loss: 0.7097058673107531, Train Reward Loss: 1.8610578544033627, Test State Loss: 0.41799604484811426, Test Reward Loss: 3.1181484673544766\n",
      "Epoch 240: Train State Loss: 0.5401957600519527, Train Reward Loss: 1.8588417152387504, Test State Loss: 0.30270540290512143, Test Reward Loss: 3.12691889733891\n",
      "Epoch 250: Train State Loss: 0.46280355932144446, Train Reward Loss: 1.8577897528721832, Test State Loss: 0.28060541601851585, Test Reward Loss: 3.123723780263099\n",
      "Epoch 260: Train State Loss: 0.4758812042272184, Train Reward Loss: 1.8552756027075505, Test State Loss: 0.20261302036698908, Test Reward Loss: 3.0712978335529986\n",
      "Epoch 270: Train State Loss: 0.46010038174153306, Train Reward Loss: 1.8536715582079923, Test State Loss: 0.2986615848727524, Test Reward Loss: 3.07047070525412\n",
      "Epoch 280: Train State Loss: 0.4479931230485672, Train Reward Loss: 1.849240704567837, Test State Loss: 0.16372861644485964, Test Reward Loss: 3.077921844643133\n",
      "Epoch 290: Train State Loss: 0.4510231813522987, Train Reward Loss: 1.849950562198594, Test State Loss: 0.2764026488387026, Test Reward Loss: 3.0622100419532217\n",
      "Epoch 300: Train State Loss: 0.2853197117176605, Train Reward Loss: 1.8584133824326836, Test State Loss: 0.3434573346748948, Test Reward Loss: 3.062789264399507\n",
      "Epoch 310: Train State Loss: 0.46182107854390053, Train Reward Loss: 1.8551531716501457, Test State Loss: 0.24730598347261548, Test Reward Loss: 3.0743518259983422\n",
      "Epoch 320: Train State Loss: 0.3894991421707673, Train Reward Loss: 1.8469877394696004, Test State Loss: 0.22446620068047196, Test Reward Loss: 3.086012974799378\n",
      "Epoch 330: Train State Loss: 0.3446112209734274, Train Reward Loss: 1.8585008129908538, Test State Loss: 0.16746961154509335, Test Reward Loss: 3.1448825065317942\n",
      "Epoch 340: Train State Loss: 0.3772808988897596, Train Reward Loss: 1.8488460084396496, Test State Loss: 0.1605420111771673, Test Reward Loss: 3.102316991347934\n",
      "Epoch 350: Train State Loss: 0.34514192265982274, Train Reward Loss: 1.8475104322016267, Test State Loss: 0.2281914717517793, Test Reward Loss: 3.080657324279309\n",
      "Epoch 360: Train State Loss: 0.4380425380793167, Train Reward Loss: 1.852170822540495, Test State Loss: 0.29093937187455593, Test Reward Loss: 3.100305209376255\n",
      "Epoch 370: Train State Loss: 0.40268363410886376, Train Reward Loss: 1.8477730508336958, Test State Loss: 0.48645796892698856, Test Reward Loss: 3.106381319535412\n",
      "Epoch 380: Train State Loss: 0.37410826383018864, Train Reward Loss: 1.8470220258909844, Test State Loss: 0.14774136088322848, Test Reward Loss: 3.075345161902951\n",
      "Epoch 390: Train State Loss: 0.34539366299961693, Train Reward Loss: 1.8368427186483516, Test State Loss: 0.17256593375001103, Test Reward Loss: 3.068368657817482\n",
      "Epoch 400: Train State Loss: 0.3565677543494385, Train Reward Loss: 1.8368215991441765, Test State Loss: 0.18263676999602466, Test Reward Loss: 3.051307509106655\n",
      "Epoch 410: Train State Loss: 0.3284716963400133, Train Reward Loss: 1.834608842848828, Test State Loss: 0.15298554742708803, Test Reward Loss: 3.0370693570595177\n",
      "Epoch 420: Train State Loss: 0.3129036950699519, Train Reward Loss: 1.8278295581534842, Test State Loss: 0.391933628753759, Test Reward Loss: 3.0546393716108287\n",
      "Epoch 430: Train State Loss: 0.36293681927328, Train Reward Loss: 1.8420612339486844, Test State Loss: 0.16747135343961417, Test Reward Loss: 3.0773147829969094\n",
      "Epoch 440: Train State Loss: 0.3025105427366216, Train Reward Loss: 1.8391617613869788, Test State Loss: 0.23584555437788368, Test Reward Loss: 3.08222659151048\n",
      "Epoch 450: Train State Loss: 0.37775917929247954, Train Reward Loss: 1.8353766036958141, Test State Loss: 0.26093987399945034, Test Reward Loss: 3.080202490118377\n",
      "Epoch 460: Train State Loss: 0.342898320866283, Train Reward Loss: 1.8363048040517864, Test State Loss: 0.34897474652621896, Test Reward Loss: 3.0902003146994685\n",
      "Epoch 470: Train State Loss: 0.20691594615130451, Train Reward Loss: 1.824680833620587, Test State Loss: 0.24403878826647996, Test Reward Loss: 3.0478393633656378\n",
      "Epoch 480: Train State Loss: 0.39707260971097275, Train Reward Loss: 1.8407875017844066, Test State Loss: 0.22618141188286245, Test Reward Loss: 3.0937213840055118\n",
      "Epoch 490: Train State Loss: 0.3476745109257754, Train Reward Loss: 1.8336948281803525, Test State Loss: 0.20014872767962516, Test Reward Loss: 3.0501940272609773\n",
      "Epoch 500: Train State Loss: 0.3185721219799016, Train Reward Loss: 1.8289061148964603, Test State Loss: 0.20833498106803744, Test Reward Loss: 3.0408968034525787\n",
      "Epoch 510: Train State Loss: 0.3167631216010195, Train Reward Loss: 1.8240439442118113, Test State Loss: 0.24120630711782723, Test Reward Loss: 3.0383879947001695\n",
      "Epoch 520: Train State Loss: 0.3393379817301175, Train Reward Loss: 1.841054316798262, Test State Loss: 0.24838782342150809, Test Reward Loss: 3.0411234778786094\n",
      "Epoch 530: Train State Loss: 0.30558531006984413, Train Reward Loss: 1.8296950490133048, Test State Loss: 0.23124740863684565, Test Reward Loss: 3.038359290210865\n",
      "Epoch 540: Train State Loss: 0.3004627836056752, Train Reward Loss: 1.8339523261049855, Test State Loss: 0.17586056346073747, Test Reward Loss: 3.041604432005315\n",
      "Epoch 550: Train State Loss: 0.3618823605382349, Train Reward Loss: 1.8360264034730824, Test State Loss: 0.13159848228096963, Test Reward Loss: 3.0601055578703016\n",
      "Epoch 560: Train State Loss: 0.28695044889044946, Train Reward Loss: 1.8234634858662455, Test State Loss: 0.22665150033310055, Test Reward Loss: 3.0275875774568615\n",
      "Epoch 570: Train State Loss: 0.2911261889140587, Train Reward Loss: 1.8241000278216757, Test State Loss: 0.19931021827505901, Test Reward Loss: 3.0282994995159425\n",
      "Epoch 580: Train State Loss: 0.2844609024784295, Train Reward Loss: 1.8247988041142975, Test State Loss: 0.14584154992364348, Test Reward Loss: 3.0281760108555176\n",
      "Epoch 590: Train State Loss: 0.3354072607040871, Train Reward Loss: 1.8235059696568217, Test State Loss: 0.16486148751224391, Test Reward Loss: 3.0310839506883305\n",
      "Epoch 600: Train State Loss: 0.2992851565890014, Train Reward Loss: 1.8259802317451181, Test State Loss: 0.2104618356633, Test Reward Loss: 3.038004111408845\n",
      "Epoch 610: Train State Loss: 0.2399440447797533, Train Reward Loss: 1.834433579428121, Test State Loss: 0.2118498874036595, Test Reward Loss: 3.0660994048434986\n",
      "Epoch 620: Train State Loss: 0.3437788399285637, Train Reward Loss: 1.8241344944657902, Test State Loss: 0.21428568627219646, Test Reward Loss: 3.0441792154676532\n",
      "Epoch 630: Train State Loss: 0.3282069556646747, Train Reward Loss: 1.8368069611240267, Test State Loss: 0.18336962343659252, Test Reward Loss: 3.0380585895281143\n",
      "Epoch 640: Train State Loss: 0.32588511843269224, Train Reward Loss: 1.8355855262720298, Test State Loss: 0.21117269509937614, Test Reward Loss: 3.0339326833787914\n",
      "Epoch 650: Train State Loss: 0.28873214825004107, Train Reward Loss: 1.8280450653446383, Test State Loss: 0.5226528485026211, Test Reward Loss: 3.0276450060517526\n",
      "Epoch 660: Train State Loss: 0.30513312128174586, Train Reward Loss: 1.8282790242696247, Test State Loss: 0.15332365417852997, Test Reward Loss: 3.03013046110221\n",
      "Epoch 670: Train State Loss: 0.29711627440323357, Train Reward Loss: 1.8265155607578085, Test State Loss: 0.15004203758202495, Test Reward Loss: 3.047568606060231\n",
      "Epoch 680: Train State Loss: 0.26677974766108675, Train Reward Loss: 1.825311269838951, Test State Loss: 0.2236437677964568, Test Reward Loss: 3.053299969802565\n",
      "Epoch 690: Train State Loss: 0.31672898352262563, Train Reward Loss: 1.8294492740138741, Test State Loss: 0.12829531421419232, Test Reward Loss: 3.044901040653949\n",
      "Epoch 700: Train State Loss: 0.26632057488779537, Train Reward Loss: 1.8235589668590373, Test State Loss: 0.15737846688833088, Test Reward Loss: 3.057810520612402\n",
      "Epoch 710: Train State Loss: 0.30566901110194156, Train Reward Loss: 1.8336160561666404, Test State Loss: 0.1586986270919442, Test Reward Loss: 3.043325359125047\n",
      "Epoch 720: Train State Loss: 0.2740488480220083, Train Reward Loss: 1.8252633939678213, Test State Loss: 0.1953421931155026, Test Reward Loss: 3.031636329829271\n",
      "Epoch 730: Train State Loss: 0.20211388288380114, Train Reward Loss: 1.816926646733578, Test State Loss: 0.18740334024769253, Test Reward Loss: 3.038430136532261\n",
      "Epoch 740: Train State Loss: 0.3668505256797653, Train Reward Loss: 1.8487560493400028, Test State Loss: 0.24959032682701945, Test Reward Loss: 3.076124747883296\n",
      "Epoch 750: Train State Loss: 0.16910659311665222, Train Reward Loss: 1.8190729805921146, Test State Loss: 0.6893707263306714, Test Reward Loss: 3.0525540504285367\n",
      "Epoch 760: Train State Loss: 0.18349645679432433, Train Reward Loss: 1.8277441293718293, Test State Loss: 0.2824281227961183, Test Reward Loss: 3.103181765458794\n",
      "Epoch 770: Train State Loss: 0.17908432088926202, Train Reward Loss: 1.8173284270501977, Test State Loss: 0.15295572038739919, Test Reward Loss: 3.032706866627159\n",
      "Epoch 780: Train State Loss: 0.2684011909314431, Train Reward Loss: 1.8227090580128937, Test State Loss: 0.15297204678878187, Test Reward Loss: 3.0346331059947262\n",
      "Epoch 790: Train State Loss: 0.3343429353123065, Train Reward Loss: 1.8239673416788633, Test State Loss: 0.2319987895851955, Test Reward Loss: 3.0261931296784312\n",
      "Epoch 800: Train State Loss: 0.4002891076314263, Train Reward Loss: 1.8309711647980165, Test State Loss: 0.37631594473030416, Test Reward Loss: 3.0790419778812064\n",
      "Epoch 810: Train State Loss: 0.21347188290953636, Train Reward Loss: 1.8224468977089363, Test State Loss: 0.5954330415604636, Test Reward Loss: 3.212518512950619\n",
      "Epoch 820: Train State Loss: 0.13072446644236332, Train Reward Loss: 1.8191405733663855, Test State Loss: 0.1860592612810433, Test Reward Loss: 3.057032141852642\n",
      "Epoch 830: Train State Loss: 0.31745271377637985, Train Reward Loss: 1.8257575613442851, Test State Loss: 0.26940068379044535, Test Reward Loss: 3.0318144437602315\n",
      "Epoch 840: Train State Loss: 0.25989781921426763, Train Reward Loss: 1.8209495015363386, Test State Loss: 0.22190910878591238, Test Reward Loss: 3.0340148039563792\n",
      "Epoch 850: Train State Loss: 0.24741328458720818, Train Reward Loss: 1.8145902082677692, Test State Loss: 0.3795524490857497, Test Reward Loss: 3.0462505925655567\n",
      "Epoch 860: Train State Loss: 0.2839957196372561, Train Reward Loss: 1.8264870738705077, Test State Loss: 0.38257032487308607, Test Reward Loss: 3.1159717987560636\n",
      "Epoch 870: Train State Loss: 0.23329121152916923, Train Reward Loss: 1.8143271996858839, Test State Loss: 0.26665447319857777, Test Reward Loss: 3.0550421654145974\n",
      "Epoch 880: Train State Loss: 0.39104578325513284, Train Reward Loss: 1.8246205810953506, Test State Loss: 0.3928407387714833, Test Reward Loss: 3.1173985973911384\n",
      "Epoch 890: Train State Loss: 0.3063849400856998, Train Reward Loss: 1.8198248085504178, Test State Loss: 0.4316633195988834, Test Reward Loss: 3.2424227617391987\n",
      "Epoch 900: Train State Loss: 0.5145506282111164, Train Reward Loss: 1.8193038077673502, Test State Loss: 0.4945696828979999, Test Reward Loss: 3.1625513685767146\n",
      "Epoch 910: Train State Loss: 0.21689216357510305, Train Reward Loss: 1.820592515072891, Test State Loss: 0.6598949491884559, Test Reward Loss: 3.3557426717335694\n",
      "Epoch 920: Train State Loss: 0.1891583692326676, Train Reward Loss: 1.8210855516113975, Test State Loss: 0.6013840206013993, Test Reward Loss: 3.3049602467287333\n",
      "Epoch 930: Train State Loss: 0.11446934403874912, Train Reward Loss: 1.815666275912633, Test State Loss: 0.23381708833854645, Test Reward Loss: 3.3508064835436744\n",
      "Epoch 940: Train State Loss: 0.17849240389501211, Train Reward Loss: 1.817539297756937, Test State Loss: 0.17659546263050288, Test Reward Loss: 3.068975906120304\n",
      "Epoch 950: Train State Loss: 0.2984392822997179, Train Reward Loss: 1.8295745353881194, Test State Loss: 0.2841343198949471, Test Reward Loss: 3.282476089365227\n",
      "Epoch 960: Train State Loss: 0.3210277785812505, Train Reward Loss: 1.818244780544345, Test State Loss: 0.3047151391790248, Test Reward Loss: 3.238825811885472\n",
      "Epoch 970: Train State Loss: 0.42204439670266586, Train Reward Loss: 1.8380184061215397, Test State Loss: 0.23100517235230655, Test Reward Loss: 3.277751974279118\n",
      "Epoch 980: Train State Loss: 0.23690921342535876, Train Reward Loss: 1.8204208862711333, Test State Loss: 0.14698330894112588, Test Reward Loss: 3.0428626993323267\n",
      "Epoch 990: Train State Loss: 0.2397281675406266, Train Reward Loss: 1.8226926483933357, Test State Loss: 0.49791197491809724, Test Reward Loss: 3.0573181576958812\n",
      "Epoch 1000: Train State Loss: 0.29671097787399775, Train Reward Loss: 1.826910998205915, Test State Loss: 0.14164323452860117, Test Reward Loss: 3.294951570096891\n",
      "Epoch 1010: Train State Loss: 0.25092973309836814, Train Reward Loss: 1.8145484340141262, Test State Loss: 0.3397063052188605, Test Reward Loss: 3.0654538419813617\n",
      "Epoch 1020: Train State Loss: 0.3653026837190846, Train Reward Loss: 1.8314930332427841, Test State Loss: 0.18183826285414398, Test Reward Loss: 3.5777363621161204\n",
      "Epoch 1030: Train State Loss: 0.2498361742119305, Train Reward Loss: 1.8197217570660267, Test State Loss: 0.20496748912148177, Test Reward Loss: 3.1864263244680116\n",
      "Epoch 1040: Train State Loss: 0.1510229036232922, Train Reward Loss: 1.8111501096358047, Test State Loss: 0.32339846245013176, Test Reward Loss: 3.4453710323830276\n",
      "Epoch 1050: Train State Loss: 0.2842933954222826, Train Reward Loss: 1.82237922653752, Test State Loss: 0.40760699001257306, Test Reward Loss: 3.4076809026663066\n",
      "Epoch 1060: Train State Loss: 0.27296794578945266, Train Reward Loss: 1.816178123382856, Test State Loss: 0.25508505542529747, Test Reward Loss: 3.3153935251072837\n",
      "Epoch 1070: Train State Loss: 0.43423068869789133, Train Reward Loss: 1.8151618473850573, Test State Loss: 0.1791105086263269, Test Reward Loss: 3.1161328282533667\n",
      "Epoch 1080: Train State Loss: 0.15653939271712441, Train Reward Loss: 1.8122201237102076, Test State Loss: 0.30747595814988016, Test Reward Loss: 3.3203731489143684\n",
      "Epoch 1090: Train State Loss: 0.25231455760938115, Train Reward Loss: 1.8210948096328028, Test State Loss: 0.20354889804497361, Test Reward Loss: 3.277091180803109\n",
      "Epoch 1100: Train State Loss: 0.17925834645784924, Train Reward Loss: 1.8160782974431244, Test State Loss: 0.24293060264084487, Test Reward Loss: 3.2564190735433294\n",
      "Epoch 1110: Train State Loss: 0.379019693777489, Train Reward Loss: 1.8216550897047012, Test State Loss: 0.2085772506520152, Test Reward Loss: 3.4573665612094917\n",
      "Epoch 1120: Train State Loss: 0.18795940303290262, Train Reward Loss: 1.8151306810021859, Test State Loss: 0.20502152058761566, Test Reward Loss: 3.3588689531618368\n",
      "Epoch 1130: Train State Loss: 0.2778974955735612, Train Reward Loss: 1.8206473336864, Test State Loss: 0.14435634056106209, Test Reward Loss: 3.355409122042347\n",
      "Epoch 1140: Train State Loss: 0.33619843209022654, Train Reward Loss: 1.8212748978265132, Test State Loss: 0.709649043653626, Test Reward Loss: 3.203638376278127\n",
      "Epoch 1150: Train State Loss: 0.166468843186507, Train Reward Loss: 1.8090606468518355, Test State Loss: 0.40356491699581964, Test Reward Loss: 3.4044119734088962\n",
      "Epoch 1160: Train State Loss: 0.36349101304123177, Train Reward Loss: 1.8287529826777795, Test State Loss: 0.26648943743668496, Test Reward Loss: 3.3363480476558105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m total_reward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, r \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     xx \u001b[38;5;241m=\u001b[39m state_encoder(x)\n\u001b[0;32m      9\u001b[0m     pred_state \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m@\u001b[39m xx\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:406\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\inspect.py:826\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikef\\anaconda3\\envs\\gym\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train, test are lists of tuples of (x, y, r)\n",
    "epochs = 10000\n",
    "criterion = torch.nn.MSELoss()\n",
    "for i in range(epochs):\n",
    "    total_state_loss = 0\n",
    "    total_reward_loss = 0\n",
    "    for x, y, r in train:\n",
    "        optimizer.zero_grad()\n",
    "        xx = state_encoder(x)\n",
    "        pred_state = A @ xx\n",
    "        Q_psd = Q.T @ Q\n",
    "        pred_reward = xx.T @ Q_psd @ xx\n",
    "        dec_state = state_decoder(pred_state)   \n",
    "        state_loss = criterion(dec_state, y)\n",
    "        reward_loss = criterion(pred_reward, r)\n",
    "        loss = state_loss + reward_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_state_loss += state_loss.item()\n",
    "        total_reward_loss += reward_loss.item()\n",
    "    if i % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            total_test_state_loss = 0\n",
    "            total_test_reward_loss = 0\n",
    "            for x, y,r  in test:\n",
    "                xx = state_encoder(x)\n",
    "                pred_state = A @ xx\n",
    "                Q_psd = Q.T @ Q\n",
    "                pred_reward = xx.T @ Q_psd @ xx\n",
    "                dec_state = state_decoder(pred_state)\n",
    "                state_loss = criterion(dec_state, y)\n",
    "                reward_loss = criterion(pred_reward, r)\n",
    "                total_test_state_loss += state_loss.item()\n",
    "                total_test_reward_loss += reward_loss.item()\n",
    "            print(f\"Epoch {i}: Train State Loss: {total_state_loss/N}, Train Reward Loss: {total_reward_loss/N}, Test State Loss: {total_test_state_loss/len(test)}, Test Reward Loss: {total_test_reward_loss/len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3892, -1.1420,  0.4347,  0.3648])\n",
      "tensor([ 0.0351, -0.0070,  0.1394, -1.6561,  0.3626, -0.8260,  1.1684,  0.1074,\n",
      "         0.7020, -0.0528, -0.3366,  0.7623, -0.2779, -0.1570,  0.3622,  0.0454],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([ 0.1514, -1.4894,  0.0357,  0.3568])\n",
      "tensor([ 0.1218, -1.5987,  0.1711,  0.1662], grad_fn=<ViewBackward0>)\n",
      "Test\n",
      "tensor([ 0.6592, -0.8172, -1.5943, -2.5283])\n",
      "tensor([ 0.4345, -0.5458,  6.4610, -0.5755])\n",
      "tensor([ 0.3680, -0.9164,  6.7711, -1.4532], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = train[0]\n",
    "x, y, r = t\n",
    "xx = state_encoder(x)\n",
    "pred_state = A @ xx\n",
    "dec_state = state_decoder(pred_state)\n",
    "print(x)\n",
    "print(xx)\n",
    "print(y)\n",
    "print(dec_state)\n",
    "print(\"Test\")\n",
    "t = test[0]\n",
    "x, y,r = t\n",
    "xx = state_encoder(x)\n",
    "pred_state = A @ xx\n",
    "dec_state = state_decoder(pred_state)\n",
    "print(x)\n",
    "print(y)\n",
    "print(dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
